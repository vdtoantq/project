# -*- coding: utf-8 -*-
"""demo_cell_segment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16UkHFvMzOu6hWVMHdPqGrh3Ra6TWPiF2

Code demo dùng Unet

Code này gồm phần đọc dữ liệu và thực thi trên Unet,
 là một phần trong Đồ án 1 của Minh và Hùng với cell segmentation. 
 Trong đồ án có trình bày nhiều phương pháp so sánh,
  trong đó Unet là phương pháp cho kết quả không tốt 
  bằng các phương pháp mới. Tuy nhiên đây là phương pháp cơ bản, phù hợp cho các bạn mới tiếp cận

# Nhập dữ liệu đầu vào

Bộ dữ liệu này chứa một số lượng lớn các hình phân đoạn nhân tế bào. Hình ảnh thu được trong nhiều điều kiện khác nhau và khác nhau về loại tế bào, độ phóng đại và phương thức hình ảnh. Tập dữ liệu được thiết kế để thách thức khả năng tổng quát hóa của thuật toán đối với các biến thể khác nhau

Notebook này được thiết kế để bất kì ai cũng có thể truy cập vào data và chạy thử các model

## Tạo thư mục và lưu data

Lưu ý: Tạo 1 thư mục DA1 trong drive của bạn
"""

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# import thư viện cần thiết
import os
from tqdm import tqdm
import numpy as np
import cv2
import matplotlib.pyplot as plt
import random
import sys
from skimage.io import imread, imshow
from skimage.transform import resize

# Xác thực và tạo 1 driveClient
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# Tải các file từ đường dẫn drive về mục Files của colab để sử dụng
id_idx = 5
# train
train_link = "https://drive.google.com/file/d/1KQng4k9Mx-bjpPbg2_03vU8M3OCbc90S/view?usp=sharing"
train_id = train_link.split("/")[id_idx]
train_file = drive.CreateFile({'id':train_id})
train_file.GetContentFile("stage1_train.zip")

"""
file test chua 65 anh nhung khong co mask, nen ta trich tam tu file train de lam tap test
"""
# # test
# test_link = "https://drive.google.com/file/d/1vQ6uFSxWqYNwXXfSlMyeotTyWrva2kR4/view?usp=sharing"
# test_id = test_link.split("/")[id_idx]
# test_file = drive.CreateFile({'id':test_id})
# test_file.GetContentFile("stage1_test.zip")

# # final_test
# final_test_link = "https://drive.google.com/file/d/1IZGWPy61DyDqd-TFwdFDOUwKSjURwo-R/view?usp=sharing"
# final_test_id = final_test_link.split("/")[id_idx]
# final_test_file = drive.CreateFile({'id':final_test_id})
# final_test_file.GetContentFile("stage2_test_final.zip")

# Tạo thư mục và giải nén các file vừa được thêm vào
from zipfile import ZipFile

with ZipFile('stage1_train.zip', 'r') as zipObj:
   zipObj.extractall("/content/train")

# with ZipFile('stage1_test.zip', 'r') as zipObj:
#    zipObj.extractall("/content/test")

# with ZipFile('stage2_test_final.zip', 'r') as zipObj:
#    zipObj.extractall("/content/final_test")

"""Lúc này ta có 2 thư mục là train và test

Thư mục train chứa các folder, mỗi folder là có 2 folder con là images và masks. Thư mục images chứa ảnh nuclei, thư mục masks chứa tập hợp rất nhiều ảnh mà mỗi ảnh chứa một segmented nuclei. Điều này khiến cho việc train đôi lúc gặp bất tiện và ta phải xử lí chúng.

## constant
"""

seed = 1234
np.random.seed = seed

TRAIN_PATH = "/content/train/"
TEST_PATH = "/content/test/"
FINAL_TEST_PATH = "/content/final_test/"
IMG_WIDTH = 256
IMG_HEIGHT = 256
NUM_CHANNEL = 3

"""## Đọc dữ liệu"""

# Khởi tạo các biến chứa tên các thư mục con trong tập train và test
train_ids = next(os.walk(TRAIN_PATH))[1]
# test_ids  = next(os.walk(TEST_PATH ))[1]
# final_test_ids = next(os.walk(FINAL_TEST_PATH))[1]

# Train image
X_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, NUM_CHANNEL), dtype=np.uint8)
Y_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)

print("loading_data ...")
for n, id_ in enumerate(train_ids):   
    path = TRAIN_PATH + id_
    img = imread(path + '/images/' + id_ + '.png')[:,:,:NUM_CHANNEL]  
    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)
    X_train[n] = img  #Fill empty X_train with values from img
    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)
    for mask_file in next(os.walk(path + '/masks/'))[2]:
        mask_ = imread(path + '/masks/' + mask_file)
        mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant',  
                                      preserve_range=True), axis=-1)
        mask = np.maximum(mask, mask_)  
            
    Y_train[n] = mask  
Y_train = np.asarray(Y_train,dtype = np.float) 
X_train  = np.asarray(X_train,dtype= np.float)
print('Done!')
print("X_train dimension : {}".format(X_train.shape))
print("Y_train dimension : {}".format(Y_train.shape))

# Test image
# nếu cần load file test thì uncomment đoạn này
"""
X_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, NUM_CHANNEL), dtype=np.uint8)
sizes_test = []
print('Resizing test images') 
for n, id_ in enumerate(test_ids):
    path = TEST_PATH + id_
    img = imread(path + '/images/' + id_ + '.png')[:,:,:NUM_CHANNEL]
    sizes_test.append([img.shape[0], img.shape[1]])
    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)
    X_test[n] = img
print("X_test dimension : {}".format(X_test.shape))
print('Done!')
"""

# Nếu cần load file test_final thì uncomment đoạn này
"""
X_test_final = np.zeros((len(final_test_ids), IMG_HEIGHT, IMG_WIDTH, NUM_CHANNEL), dtype=np.uint8)
sizes_test = []
print('Resizing final test images') 
for n, id_ in enumerate(final_test_ids):
    path = FINAL_TEST_PATH + id_
    img = imread(path + '/images/' + id_ + '.png')[:,:,:NUM_CHANNEL]
    sizes_test.append([img.shape[0], img.shape[1]])
    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)
    X_test_final[n] = img
print("X_test_final dimension : {}".format(X_test_final.shape))
"""

# trích khoảng 70 ảnh( khoảng 10%) làm tập test
from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test = train_test_split(X_train,Y_train,test_size = 70,random_state = 42)

print("X_train dimension : {}".format(X_train.shape))
print("Y_train dimension : {}".format(Y_train.shape))
print("X_test dimension  : {}".format(X_test.shape))
print("Y_test dimension  : {}".format(Y_test.shape))

"""# Lưu vào drive dưới dạng pickle, chỉ cần lưu một lần vào google drive của bạn.
Khi train/test với các model từ sau thì chỉ việc gọi ra
"""

# Sử dụng pickle để lưu lại đối tượng data để sử dụng sau này
import pickle

def save_file(obj, file_path):
  with open(file_path, 'wb') as f:
    pickle.dump(obj, f)
# lưu ý trong drive tạo 1 thư mục DA1
pickle_path = "/content/drive/MyDrive/DA1"

os.makedirs(pickle_path) 
# file_name = 'data_save'
# if os.path.exists(file_name):
#   shutil.rmtree(file_name)
  
# os.mkdir(file_name)


save_file(X_train,os.path.join(pickle_path,"./Xtrain.pkl"))
save_file(Y_train,os.path.join(pickle_path,"./Ytrain.pkl"))
save_file(X_test,os.path.join(pickle_path,"./Xtest.pkl"))
save_file(Y_test,os.path.join(pickle_path,"./Ytest.pkl"))
# save_file(X_test_final,os.path.join(pickle_path,".\test_final.pkl"))

"""## visualization data"""

# Visualize input data

NumSamples = 5

k = np.random.randint(len(train_ids)*0.8,size = NumSamples)
fg , ax = plt.subplots(2,NumSamples,figsize = (20,8))
fg.suptitle("Data visualization")

for i,j in enumerate(k): 
  ax[0,i].imshow(X_train[j]/255.0)
  ax[0,i].set_xlabel('image {}'.format(j))
  ax[1,i].imshow(np.squeeze(Y_train[j]))
  ax[1,i].set_xlabel('mask {}'.format(j))

"""#Train & Test with Unet

Unet model

## load file pickle
"""

# from google.colab import drive
# drive.mount('/content/drive')



# load dữ liệu từ pickle
import pickle
import os
def load_file(file_path):
  with open(file_path, 'rb') as f:
    obj = pickle.load(f)
  return obj

pickle_path = "/content/drive/MyDrive/DA1"
X_train = load_file(os.path.join(pickle_path,"./Xtrain.pkl"))
Y_train = load_file(os.path.join(pickle_path,"./Ytrain.pkl"))
X_test = load_file(os.path.join(pickle_path,"./Xtest.pkl"))
Y_test = load_file(os.path.join(pickle_path,"./Ytest.pkl"))

print("X_train dimension : {}".format(X_train.shape))
print("Y_train dimension : {}".format(Y_train.shape))
print("X_test dimension : {}".format(X_test.shape))
print("Y_test dimension : {}".format(Y_test.shape))

"""## import thư viện cần dùng"""

# import thư viện
import tensorflow as tf 
from keras import backend as K
import numpy as np
import matplotlib.pyplot as plt
import random
import cv2 
from keras.callbacks import Callback
from IPython.display import clear_output
from tensorflow.keras.layers import*
from tensorflow.keras.models import Model
from tensorflow.keras.applications import*

"""## hyper parameter"""

# hyper parameter
IMG_WIDTH = 256
IMG_HEIGHT = 256
NUM_CHANNEL = 3
BATCH_SIZE = 16
EPOCH = 300
learning_rate = 1e-4
Validation_split = 0.1
save_path = "/content/drive/MyDrive/DA1/Unet.h5"
seed = 1234
np.random.seed = seed

"""## custom metric"""

# custom metric
def IoU(y_true, y_pred, smooth=0):
  intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])
  union = K.sum(y_true,[1,2,3])+K.sum(y_pred,[1,2,3])-intersection
  iou = K.mean((intersection + smooth) / (union + smooth), axis=0)
  return iou

def dice_coef(y_true, y_pred, smooth=0.0):
    '''Average dice coefficient per batch.'''
    axes = (1,2,3)
    intersection = K.sum(y_true * y_pred, axis=axes)
    summation = K.sum(y_true + y_pred, axis=axes)
    
    return K.mean((2.0 * intersection + smooth) / (summation + smooth), axis=0)
    
def dice_coef_loss(y_true, y_pred):
    return 1.0 - dice_coef(y_true, y_pred, 0)
    #return 1.0 - dice_coef(y_true, y_pred)
    
def jaccard_coef(y_true, y_pred, smooth=0.0):
    '''Average jaccard coefficient per batch.'''
    axes = (1,2,3)
    intersection = K.sum(y_true * y_pred, axis=axes)
    union = K.sum(y_true + y_pred, axis=axes) - intersection
    return K.mean( (intersection + smooth) / (union + smooth), axis=0)

"""## define model"""

# define model
def Unet():
  inputs = Input((IMG_HEIGHT, IMG_WIDTH, NUM_CHANNEL))
  s = tf.keras.layers.Lambda(lambda x: x / 255.0)(inputs)

  #Contraction path
  c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)
  c1 = Dropout(0.1)(c1)
  c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)
  p1 = MaxPooling2D((2, 2))(c1)

  c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)
  c2 = Dropout(0.1)(c2)
  c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)
  p2 = MaxPooling2D((2, 2))(c2)
 
  c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)
  c3 = Dropout(0.2)(c3)
  c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)
  p3 = MaxPooling2D((2, 2))(c3)
 
  c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)
  c4 = Dropout(0.2)(c4)
  c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)
  p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)
 
  c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)
  c5 = Dropout(0.3)(c5)
  c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)

  #Expansive path 
  u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)
  u6 = concatenate([u6, c4])
  c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)
  c6 = Dropout(0.2)(c6)
  c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)
 
  u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)
  u7 = concatenate([u7, c3])
  c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)
  c7 = Dropout(0.2)(c7)
  c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)
 
  u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)
  u8 = concatenate([u8, c2])
  c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)
  c8 = Dropout(0.1)(c8)
  c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)
 
  u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)
  u9 = concatenate([u9, c1], axis=3)
  c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)
  c9 = Dropout(0.1)(c9)
  c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)
 
  outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)
 
  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])
  optimizer = tf.keras.optimizers.Adam(lr = learning_rate)
  model.compile(optimizer=optimizer, loss=dice_coef_loss, metrics=["accuracy",dice_coef,jaccard_coef,IoU,])
  return model

"""## callbacks"""

# plot losses
class PlotLosses(tf.keras.callbacks.Callback):
  def on_train_begin(self, logs={}):
    self.i = 0
    self.x = []
    # losses
    self.losses = []
    self.val_losses = []

    # accuracy
    self.accuracy = []
    self.val_accuracy = []

    # dice_coef
    self.dice_coef = []
    self.val_dice_coef = []

    # IoU
    self.IoU = []
    self.val_IoU = []
        
    self.fig = plt.figure()
    self.logs = []

  def on_epoch_end(self, epoch, logs={}):
        
    self.logs.append(logs)
    self.x.append(self.i)
    self.losses.append(logs.get('loss'))
    self.val_losses.append(logs.get('val_loss'))

    self.accuracy.append(logs.get('accuracy'))
    self.val_accuracy.append(logs.get('val_accuracy'))

    self.dice_coef.append(logs.get('dice_coef'))
    self.val_dice_coef.append(logs.get('val_dice_coef'))

    self.IoU.append(logs.get('IoU'))
    self.val_IoU.append(logs.get('val_IoU'))
    self.i += 1
        
    clear_output(wait=True)

    fg , ax = plt.subplots(2,2,figsize = (16,8))
    fg.suptitle("Results training after {} epochs".format(self.i))

    ax[0,0].plot(self.x,self.losses,label = 'loss')
    ax[0,0].plot(self.x,self.val_losses,label = 'val_losses')
    ax[0,0].set_xlabel("min loss {}--min val_loss {}".format(np.round(min(self.losses),2),np.round(min(self.val_losses),2)))
    # ax[0,0].set_xlabel("losses")
    ax[0,0].legend()
        
    ax[0,1].plot(self.x,self.accuracy,label = 'accuracy')
    ax[0,1].plot(self.x,self.val_accuracy,label = 'val_accuracy')
    ax[0,1].set_xlabel("max accuracy {}--max val_recall {}".format(np.round(max(self.accuracy),2),np.round(max(self.val_accuracy),2)))
    # ax[0,1].set_xlabel("accuracy")
    ax[0,1].legend()

    ax[1,0].plot(self.x,self.dice_coef,label = 'dice_coef')
    ax[1,0].plot(self.x,self.val_dice_coef,label = 'val_dice_coef')
    ax[1,0].set_xlabel("max dice_coef {}--max val_dice_coef {}".format(np.round(max(self.dice_coef),2),np.round(max(self.val_dice_coef),2)))
    # ax[1,0].set_xlabel("dice_coef")
    ax[1,0].legend()

    ax[1,1].plot(self.x,self.IoU,label = 'IoU')
    ax[1,1].plot(self.x,self.val_IoU,label = 'val_IoU')
    ax[1,1].set_xlabel("max IoU {}--max val_IoU {}".format(np.round(max(self.IoU),2),np.round(max(self.val_IoU),2)))
    # ax[1,1].set_xlabel("IoU")
    ax[1,1].legend()

    plt.show()
        
plot_losses = PlotLosses()

# callbacks
callbacks = [
        tf.keras.callbacks.EarlyStopping(patience=50, monitor="accuracy"),
        tf.keras.callbacks.ModelCheckpoint(save_path, verbose=2, save_best_only=True),
        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=20, min_lr=0.00001),
        plot_losses
            ]

# data augumentation
data_gen_args = dict(rotation_range=0.2,
                    width_shift_range=0.05,
                    height_shift_range=0.05,
                    shear_range=0.05,
                    zoom_range=0.05,
                    horizontal_flip=True,
                    fill_mode='nearest')

"""## compile and train"""

model = Unet()
model.summary()

# train
History = model.fit(X_train, Y_train, validation_split=Validation_split, batch_size=BATCH_SIZE, epochs=EPOCH, callbacks=callbacks)

results = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, callbacks=callbacks)
print("loss : {} ".format(results[0]))
print("acc  : {} ".format(results[1]))
print("dice : {} ".format(results[2]))
print("IoU  : {} ".format(results[4]))

"""## visualize result"""

new_model = Unet()
new_model.load_weights(save_path)

idx = random.randint(0, len(X_train))


# preds_train = new_model.predict(X_train[:int(X_train.shape[0]*0.9)], verbose=1)
# preds_val = new_model.predict(X_train[int(X_train.shape[0]*0.9):], verbose=1)
preds_test = new_model.predict(X_test, verbose=1)

 
# preds_train_t = (preds_train > 0.5).astype(np.float)
# preds_val_t = (preds_val > 0.5).astype(np.float)
preds_test_t = (preds_test > 0.5).astype(np.float)

size_samples = 5
random_list = np.random.randint(len(preds_test_t),size = size_samples)
fg , ax = plt.subplots(size_samples,3,figsize = (20,20))
fg.suptitle("Compare result training set")

for stt,ix in enumerate(random_list):
  ax[stt,0].imshow(X_test[ix]/255.0)
  ax[stt,0].set_xlabel("original image {}".format(ix))
  ax[stt,1].imshow(np.squeeze(Y_test[ix]))
  ax[stt,1].set_xlabel("mask: IoU|Dice")
  ax[stt,2].imshow(np.squeeze(preds_test_t[ix]))
  ax[stt,2].set_xlabel("predict {}|{}".format(np.round(IoU(np.expand_dims(Y_test[ix],axis = 0),np.expand_dims(preds_test_t[ix],axis = 0)),3),
                                            np.round(dice_coef(np.expand_dims(Y_test[ix],axis = 0),np.expand_dims(preds_test_t[ix],axis = 0)),3)))
plt.show()

"""# Unet++ (chỉ nên đọc và chạy sau khi đã hiểu và thực thi thành thạo Unet): các bước thực hiện tương tự"""

from google.colab import drive
drive.mount('/content/drive')

# load dữ liệu từ pickle
import pickle
import os
def load_file(file_path):
  with open(file_path, 'rb') as f:
    obj = pickle.load(f)
  return obj

pickle_path = "/content/drive/MyDrive/DA1"
X_train = load_file(os.path.join(pickle_path,"./Xtrain.pkl"))
Y_train = load_file(os.path.join(pickle_path,"./Ytrain.pkl"))
X_test = load_file(os.path.join(pickle_path,"./Xtest.pkl"))
Y_test = load_file(os.path.join(pickle_path,"./Ytest.pkl"))

# import thư viện
import keras
import tensorflow as tf 
from keras import backend as K
import numpy as np
import matplotlib.pyplot as plt
import random
import cv2 
import os
from keras.callbacks import Callback
from IPython.display import clear_output
from tensorflow.keras.layers import*
from tensorflow.keras.models import Model
from tensorflow.keras.applications import*
from tensorflow.keras.regularizers import l2

# hyper parameter
IMG_WIDTH = 256
IMG_HEIGHT = 256
NUM_CHANNEL = 3
BATCH_SIZE = 16
EPOCH = 300
learning_rate = 1e-3
Validation_split = 0.1
save_path = "/content/drive/MyDrive/DA1/"
unet_plus2_path = os.path.join(save_path,"Standard_Unet_pp3.h5")
seed = 1234
np.random.seed = seed

# plot losses
class PlotLosses(tf.keras.callbacks.Callback):
  def on_train_begin(self, logs={}):
    self.i = 0
    self.x = []
    # losses
    self.losses = []
    self.val_losses = []

    # accuracy
    self.accuracy = []
    self.val_accuracy = []

    # dice_coef
    self.dice_coef = []
    self.val_dice_coef = []

    # IoU
    self.IoU = []
    self.val_IoU = []
        
    self.fig = plt.figure()
    self.logs = []

  def on_epoch_end(self, epoch, logs={}):
        
    self.logs.append(logs)
    self.x.append(self.i)
    self.losses.append(logs.get('loss'))
    self.val_losses.append(logs.get('val_loss'))

    self.accuracy.append(logs.get('accuracy'))
    self.val_accuracy.append(logs.get('val_accuracy'))

    self.dice_coef.append(logs.get('dice_coef'))
    self.val_dice_coef.append(logs.get('val_dice_coef'))

    self.IoU.append(logs.get('IoU'))
    self.val_IoU.append(logs.get('val_IoU'))
    self.i += 1
        
    clear_output(wait=True)

    fg , ax = plt.subplots(2,2,figsize = (16,8))
    fg.suptitle("Results training after {} epochs".format(self.i))

    ax[0,0].plot(self.x,self.losses,label = 'loss')
    ax[0,0].plot(self.x,self.val_losses,label = 'val_losses')
    ax[0,0].set_xlabel("min loss {}--min val_loss {}".format(np.round(min(self.losses),2),np.round(min(self.val_losses),2)))
    # ax[0,0].set_xlabel("losses")
    ax[0,0].legend()
        
    ax[0,1].plot(self.x,self.accuracy,label = 'accuracy')
    ax[0,1].plot(self.x,self.val_accuracy,label = 'val_accuracy')
    ax[0,1].set_xlabel("max accuracy {}--max val_recall {}".format(np.round(max(self.accuracy),2),np.round(max(self.val_accuracy),2)))
    # ax[0,1].set_xlabel("accuracy")
    ax[0,1].legend()

    ax[1,0].plot(self.x,self.dice_coef,label = 'dice_coef')
    ax[1,0].plot(self.x,self.val_dice_coef,label = 'val_dice_coef')
    ax[1,0].set_xlabel("max dice_coef {}--max val_dice_coef {}--val_dice :{}".format(np.round(max(self.dice_coef),2),np.round(max(self.val_dice_coef),2),np.round(self.val_dice_coef[-1],2)))
    # ax[1,0].set_xlabel("dice_coef")
    ax[1,0].legend()

    ax[1,1].plot(self.x,self.IoU,label = 'IoU')
    ax[1,1].plot(self.x,self.val_IoU,label = 'val_IoU')
    ax[1,1].set_xlabel("max IoU {}--max val_IoU {}--val_iou :{}".format(np.round(max(self.IoU),2),np.round(max(self.val_IoU),2),np.round(self.val_IoU[-1],2)))
    # ax[1,1].set_xlabel("IoU")
    ax[1,1].legend()

    plt.show()
        
plot_losses = PlotLosses()

# custom metric
smooth = 1.
def IoU(y_true, y_pred, smooth=smooth):
  intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])
  union = K.sum(y_true,[1,2,3])+K.sum(y_pred,[1,2,3])-intersection
  iou = K.mean((intersection + smooth) / (union + smooth), axis=0)
  return iou
def dice_coef(y_true, y_pred):
    smooth = 1.
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

def dice_coef_loss(y_true, y_pred):
    return 1. - dice_coef(y_true, y_pred)

def bce_dice_loss(y_true, y_pred):
    return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) - dice_coef(y_true, y_pred)

# callbacks
callbacks = [
        tf.keras.callbacks.EarlyStopping(patience=30, monitor="accuracy"),
        tf.keras.callbacks.ModelCheckpoint(unet_plus2_path, verbose=2, save_best_only=True),
        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_IoU', factor=0.2, patience=20, min_lr=1e-4),
        plot_losses
            ]

"""Define model

Theo như tác giả của bài báo, đối với tập dữ liệu bowl 2018, việc lựa chọn DenseNet201 làm backbone đêm lại kết quả IoU tốt nhất

## define Unet++ model

### Standard Unet++
"""

def standard_unit(input_tensor, stage, nb_filter, kernel_size=3):

  dropout_rate = 0.5
  x = Conv2D(nb_filter, (kernel_size, kernel_size), name='conv'+stage+'_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(input_tensor)
  # x = BatchNormalization()(x)
  x = Activation('relu')(x)
  # x = LeakyReLU(alpha = 0.2)(x)
  x = Dropout(dropout_rate, name='dp'+stage+'_1')(x)

  x = Conv2D(nb_filter, (kernel_size, kernel_size), name='conv'+stage+'_2', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(x)
  # x = BatchNormalization()(x)
  x = Activation('relu')(x)
  # x = LeakyReLU(alpha = 0.2)(x)
  x = Dropout(dropout_rate, name='dp'+stage+'_2')(x)

  return x

def Unet_plus_plus(deep_supervision = False):
  # parameter
  nb_filter = [32,64,128,256,512]
  bn_axis = -1


  inputs = Input((IMG_HEIGHT, IMG_WIDTH, NUM_CHANNEL))
  s = tf.keras.layers.Lambda(lambda x: x / 255.0)(inputs)

  conv1_1 = standard_unit(s,'11',nb_filter[0])
  pool1 = MaxPooling2D((2, 2), strides=(2, 2), name='pool1')(conv1_1)

  conv2_1 = standard_unit(pool1, '21', nb_filter[1])
  pool2 = MaxPooling2D((2, 2), strides=(2, 2), name='pool2')(conv2_1)

  up1_2 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up12', padding='same')(conv2_1)
  conv1_2 = concatenate([up1_2, conv1_1], name='merge12', axis=bn_axis)
  conv1_2 = standard_unit(conv1_2, '12',nb_filter[0])

  conv3_1 = standard_unit(pool2, '31', nb_filter[2])
  pool3 = MaxPooling2D((2, 2), strides=(2, 2), name='pool3')(conv3_1)

  up2_2 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up22', padding='same')(conv3_1)
  conv2_2 = concatenate([up2_2, conv2_1], name='merge22', axis=bn_axis)
  conv2_2 = standard_unit(conv2_2,'22', nb_filter[1])

  up1_3 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up13', padding='same')(conv2_2)
  conv1_3 = concatenate([up1_3, conv1_1, conv1_2], name='merge13', axis=bn_axis)
  conv1_3 = standard_unit(conv1_3,'13',nb_filter[0])

  conv4_1 = standard_unit(pool3,'41', nb_filter[3])
  pool4 = MaxPooling2D((2, 2), strides=(2, 2), name='pool4')(conv4_1)

  up3_2 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up32', padding='same')(conv4_1)
  conv3_2 = concatenate([up3_2, conv3_1], name='merge32', axis=bn_axis)
  conv3_2 = standard_unit(conv3_2, '32', nb_filter[2])

  up2_3 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up23', padding='same')(conv3_2)
  conv2_3 = concatenate([up2_3, conv2_1, conv2_2], name='merge23', axis=bn_axis)
  conv2_3 = standard_unit(conv2_3, '23', nb_filter[1])

  up1_4 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up14', padding='same')(conv2_3)
  conv1_4 = concatenate([up1_4, conv1_1, conv1_2, conv1_3], name='merge14', axis=bn_axis)
  conv1_4 = standard_unit(conv1_4, '14', nb_filter[0])

  conv5_1 = standard_unit(pool4, '51', nb_filter[4])

  up4_2 = Conv2DTranspose(nb_filter[3], (2, 2), strides=(2, 2), name='up42', padding='same')(conv5_1)
  conv4_2 = concatenate([up4_2, conv4_1], name='merge42', axis=bn_axis)
  conv4_2 = standard_unit(conv4_2, '42', nb_filter[3])

  up3_3 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up33', padding='same')(conv4_2)
  conv3_3 = concatenate([up3_3, conv3_1, conv3_2], name='merge33', axis=bn_axis)
  conv3_3 = standard_unit(conv3_3, '33', nb_filter[2])

  up2_4 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up24', padding='same')(conv3_3)
  conv2_4 = concatenate([up2_4, conv2_1, conv2_2, conv2_3], name='merge24', axis=bn_axis)
  conv2_4 = standard_unit(conv2_4, '24', nb_filter[1])

  up1_5 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up15', padding='same')(conv2_4)
  conv1_5 = concatenate([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], name='merge15', axis=bn_axis)
  conv1_5 = standard_unit(conv1_5, '15', nb_filter[0])

  nestnet_output_1 = Conv2D(1, (1, 1), activation='sigmoid', name='output_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_2)
  nestnet_output_2 = Conv2D(1, (1, 1), activation='sigmoid', name='output_2', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_3)
  nestnet_output_3 = Conv2D(1, (1, 1), activation='sigmoid', name='output_3', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_4)
  nestnet_output_4 = Conv2D(1, (1, 1), activation='sigmoid', name='output_4', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_5)

  if deep_supervision:
    output = average([nestnet_output_1,nestnet_output_2,nestnet_output_3,nestnet_output_4])
    # output = Conv2D(1, (1, 1), activation='sigmoid', name='output', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(output)
    model = Model(inputs, output)
  else:
    model = Model(inputs, nestnet_output_4)
  
  # dice_coef_loss
  # bce_dice_loss

  return model

# compile
model = Unet_plus_plus()
model.compile(loss=bce_dice_loss, 
              optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-3,beta_1=0.9, beta_2=0.99, epsilon=1e-05),
              metrics=[dice_coef,IoU,"accuracy"])
model.summary()

History = model.fit(X_train, Y_train, validation_split=Validation_split, batch_size=BATCH_SIZE, epochs=200, callbacks=callbacks)

results = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, callbacks=callbacks)
print("loss : {} ".format(results[0]))
print("acc  : {} ".format(results[3]))
print("dice : {} ".format(results[1]))
print("IoU  : {} ".format(results[2]))

"""Kết quả không thực sự tốt khi mong đợi một giá trị IoU cao hơn

.

Thử với trường hợp áp dụng deep supervision
"""

# callbacks
callbacks = [
        tf.keras.callbacks.EarlyStopping(patience=30, monitor="accuracy"),
        tf.keras.callbacks.ModelCheckpoint(os.path.join(save_path,"Unetpp_supervision.h5"), verbose=2, save_best_only=True),
        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_IoU', factor=0.5, patience=15, min_lr=1e-4),
        plot_losses
            ]

# train
model = Unet_plus_plus(deep_supervision = True)
model.compile(loss=bce_dice_loss, 
              optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-3,beta_1=0.9, beta_2=0.999, epsilon=1e-07),
              metrics=[dice_coef,IoU,"accuracy"])
History = model.fit(X_train, Y_train, validation_split=Validation_split, batch_size=BATCH_SIZE, epochs=200, callbacks=callbacks)

results = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, callbacks=callbacks)
print("loss : {} ".format(results[0]))
print("acc  : {} ".format(results[3]))
print("dice : {} ".format(results[1]))
print("IoU  : {} ".format(results[2]))

"""Kết quả gần tương tự với deep_supervision = False

### Option backbone Unet++
"""

def standard_unit(input_tensor, stage, nb_filter, kernel_size=3):

  dropout_rate = 0.5
  x = Conv2D(nb_filter, (kernel_size, kernel_size), name='conv'+stage+'_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(input_tensor)
  x = BatchNormalization()(x)
  x = Activation('relu')(x)
  # x = LeakyReLU(alpha = 0.1)(x)
  x = Dropout(dropout_rate, name='dp'+stage+'_1')(x)

  x = Conv2D(nb_filter, (kernel_size, kernel_size), name='conv'+stage+'_2', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(x)
  x = BatchNormalization()(x)
  x = Activation('relu')(x)
  # x = LeakyReLU(alpha = 0.1)(x)
  x = Dropout(dropout_rate, name='dp'+stage+'_2')(x)

  return x

def get_skip(input_tensor,backbone):
  skip_connection = []

  if backbone == "VGG19":
    base_model = VGG19(include_top=False, weights='imagenet', input_tensor=input_tensor)
    names = ["block1_conv2", "block2_conv2", "block3_conv4", "block4_conv4","block5_conv4"]
 
  if backbone == "DenseNet201":
    base_model = DenseNet201(include_top=False, weights='imagenet', input_tensor=input_tensor)
    # names = ["input_2","conv1/relu","pool2_conv","pool3_conv","pool4_conv"]
    layer_names=[layer.name for layer in base_model.layers]
    names = []
    for i in [1,4,51,139,479]:
      names.append(layer_names[i])

  for name in names:
    skip_connection.append(base_model.get_layer(name).output)

  return skip_connection

def build_model(backbone,deep_supervision = False):
  # parameter
  nb_filter = [32,64,128,256,512]
  bn_axis = -1


  inputs = Input((IMG_HEIGHT, IMG_WIDTH, NUM_CHANNEL))
  s = tf.keras.layers.Lambda(lambda x: x / 255.0)(inputs)
  skip_connection = get_skip(s,backbone)

  conv1_1 = skip_connection[0]

  conv2_1 = skip_connection[1]
  

  up1_2 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up12', padding='same')(conv2_1)
  conv1_2 = concatenate([up1_2, conv1_1], name='merge12', axis=bn_axis)
  conv1_2 = standard_unit(conv1_2, '12',nb_filter[0])

  conv3_1 = skip_connection[2]
  

  up2_2 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up22', padding='same')(conv3_1)
  conv2_2 = concatenate([up2_2, conv2_1], name='merge22', axis=bn_axis)
  conv2_2 = standard_unit(conv2_2,'22', nb_filter[1])

  up1_3 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up13', padding='same')(conv2_2)
  conv1_3 = concatenate([up1_3, conv1_1, conv1_2], name='merge13', axis=bn_axis)
  conv1_3 = standard_unit(conv1_3,'13',nb_filter[0])

  conv4_1 = skip_connection[3]
 

  up3_2 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up32', padding='same')(conv4_1)
  conv3_2 = concatenate([up3_2, conv3_1], name='merge32', axis=bn_axis)
  conv3_2 = standard_unit(conv3_2, '32', nb_filter[2])

  up2_3 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up23', padding='same')(conv3_2)
  conv2_3 = concatenate([up2_3, conv2_1, conv2_2], name='merge23', axis=bn_axis)
  conv2_3 = standard_unit(conv2_3, '23', nb_filter[1])

  up1_4 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up14', padding='same')(conv2_3)
  conv1_4 = concatenate([up1_4, conv1_1, conv1_2, conv1_3], name='merge14', axis=bn_axis)
  conv1_4 = standard_unit(conv1_4, '14', nb_filter[0])

  conv5_1 = skip_connection[4]

  up4_2 = Conv2DTranspose(nb_filter[3], (2, 2), strides=(2, 2), name='up42', padding='same')(conv5_1)
  conv4_2 = concatenate([up4_2, conv4_1], name='merge42', axis=bn_axis)
  conv4_2 = standard_unit(conv4_2, '42', nb_filter[3])

  up3_3 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up33', padding='same')(conv4_2)
  conv3_3 = concatenate([up3_3, conv3_1, conv3_2], name='merge33', axis=bn_axis)
  conv3_3 = standard_unit(conv3_3, '33', nb_filter[2])

  up2_4 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up24', padding='same')(conv3_3)
  conv2_4 = concatenate([up2_4, conv2_1, conv2_2, conv2_3], name='merge24', axis=bn_axis)
  conv2_4 = standard_unit(conv2_4, '24', nb_filter[1])

  up1_5 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up15', padding='same')(conv2_4)
  conv1_5 = concatenate([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], name='merge15', axis=bn_axis)
  conv1_5 = standard_unit(conv1_5, '15', nb_filter[0])

  nestnet_output_1 = Conv2D(1, (1, 1), activation='sigmoid', name='output_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_2)
  nestnet_output_2 = Conv2D(1, (1, 1), activation='sigmoid', name='output_2', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_3)
  nestnet_output_3 = Conv2D(1, (1, 1), activation='sigmoid', name='output_3', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_4)
  nestnet_output_4 = Conv2D(1, (1, 1), activation='sigmoid', name='output_4', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_5)

  if deep_supervision:
    output = average([nestnet_output_1,nestnet_output_2,nestnet_output_3,nestnet_output_4])
    # output = Conv2D(1, (1, 1), activation='sigmoid', name='output', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(output)
    model = Model(inputs, output)
  else:
    model = Model(inputs, nestnet_output_4)
  
  # dice_coef_loss
  # bce_dice_loss
  model.compile(loss=bce_dice_loss, 
                optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
                metrics=[dice_coef,IoU,"accuracy"])
  return model

"""#### DenseNet201"""

# callbacks
callbacks = [
        tf.keras.callbacks.EarlyStopping(patience=30, monitor="accuracy"),
        tf.keras.callbacks.ModelCheckpoint(os.path.join(save_path,"DenseNet_backbone_Unetpp.h5"), verbose=2, save_best_only=True),
        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_IoU', factor=0.2, patience=15, min_lr=1e-3),
        plot_losses,
        # tf.keras.callbacks.LearningRateScheduler(schedule, verbose=2)
            ]

model = build_model("DenseNet201")
model.summary()

History = model.fit(X_train, Y_train, validation_split=Validation_split, batch_size=BATCH_SIZE, epochs=300, callbacks=callbacks)

results = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, callbacks=callbacks)
print("loss : {} ".format(results[0]))
print("acc  : {} ".format(results[3]))
print("dice : {} ".format(results[1]))
print("IoU  : {} ".format(results[2]))